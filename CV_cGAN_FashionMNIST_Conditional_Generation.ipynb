{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 🌇 FashionMNIST 조건부 생성(cGAN) 구현 및 개선 실험\n",
        "\n",
        "\n",
        "## 1. 프로젝트 목적\n",
        "본 실습의 목적은 FashionMNIST(10개 클래스) 데이터셋에서 Conditional GAN(cGAN) 을 학습하여,\n",
        "주어진 클래스 레이블(예: T-shirt, Trouser, Sneaker 등)을 조건으로 해당 클래스의 이미지를 생성하는 모델을 구축하는 것이다.\n",
        "\n",
        "- 일반 GAN이 “그럴듯한 이미지”를 생성하는 문제라면,\n",
        "- cGAN은 ‘그럴듯함’과 동시에 ‘조건 일치(레이블 반영)’ 를 만족해야 한다.\n",
        "따라서 생성 품질뿐 아니라, 레이블-이미지 매칭이 실제로 성립하는지를 핵심 평가 대상으로 둔다.\n",
        "\n",
        "\n",
        "## 2. 데이터\n",
        "- 데이터셋: FashionMNIST (torchvision.datasets.FashionMNIST)\n",
        "- 이미지: 28×28 흑백(1채널), 10개 클래스\n",
        "- 학습/평가: train 60,000 / test 10,000\n",
        "\n",
        "\n",
        "## 3. 전체 로드맵\n",
        "\n",
        "1) 데이터 로드 및 전처리  \n",
        "- ToTensor 및 Normalize를 적용하여 입력 범위를 모델 출력 범위(tanh)와 정합되도록 설정한다.\n",
        "\n",
        "2) Baseline cGAN 구현 및 재현  \n",
        "- 조건 정보를 (noise + label embedding) 형태로 입력에 결합하는 표준적 cGAN 구조를 구현하고,\n",
        "- 학습 로그(D(x), D(G(z)), loss)의 변화를 관찰하며 학습 안정성과 한계를 확인한다.\n",
        "\n",
        "3) 개선 실험(구조 및 학습 안정화)  \n",
        "- Generator/Discriminator 구조를 더 표현력 있는 형태로 변경(Residual block 기반)\n",
        "- 손실함수 및 학습 전략을 안정화 방향으로 조정(예: LSGAN 스타일 MSE, AdamW, LR scheduler, label smoothing 등)\n",
        "\n",
        "4) 평가 및 분석  \n",
        "- 정성 평가: 클래스별 샘플 그리드로 레이블 반영 여부 및 품질 확인\n",
        "- 정량 평가(선택): FID/IS를 이용하여 체크포인트별 생성 품질 변화를 비교한다.\n",
        "\n",
        "\n",
        "## 4. 평가 기준\n",
        "- 생성 품질(선명도/다양성): 정성 평가 및 FID/IS(선택)\n",
        "- 조건 반영(클래스 일치): 클래스별 생성 결과를 고정 포맷으로 반복 점검하여 확인한다.\n",
        "\n",
        "✅ 최종 목표는 “돌아가는 GAN”이 아니라,\n",
        "\"조건부 생성이 성립하는지\"와 \"개선 전략이 실제로 품질을 올리는지\"를 근거(로그/샘플/지표)로 설명하는 것이다.\n"
      ],
      "metadata": {
        "id": "pddi9FATNABh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_S_ANE7M3Ei"
      },
      "outputs": [],
      "source": [
        "▫️임포트 & 디바이스 & 시드 고정"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install torch torchvision numpy matplotlib scipy\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 재현성(reproducibility)\n",
        "# GAN은 초기값에 따라 결과가 크게 흔들릴 수 있어서\n",
        "# 실험 비교를 하려면 seed 고정이 거의 필수\n",
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    # 아래 옵션은 속도가 조금 느려질 수 있지만, 재현성을 올립니다.\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(42)\n",
        "print(\"Seed fixed to 42\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using Device:\", device)\n"
      ],
      "metadata": {
        "id": "EsFVCwPxOvAw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "▫️데이터 로드/전처리\n",
        "\n",
        "- FashionMNIST는 흑백 28x28 이미지입니다.\n",
        "- GAN에서 자주 쓰는 전처리:\n",
        "    - ToTensor()로 [0, 1] 범위 텐서로 만들고 Normalize((0.5,), (0.5,))로 [-1, 1]로 바꿉니다.\n",
        "\n",
        "- 왜 [-1, 1]로 바꾸냐?\n",
        "    - Generator 출력에 tanh()를 쓰면 출력 범위가 [-1, 1]이라서 입력 데이터 범위도 맞추는 게 학습 안정에 유리하다."
      ],
      "metadata": {
        "id": "1M3Pdy5VO9rj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "◻️ transform + DataLoader"
      ],
      "metadata": {
        "id": "_Z0yFymGPJ_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 베이스라인 하이퍼파라미터\n",
        "image_size = 28\n",
        "num_classes = 10\n",
        "latent_dim = 100\n",
        "batch_size = 64\n",
        "num_epochs = 200\n",
        "lr = 0.0002\n",
        "beta1, beta2 = 0.5, 0.999\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))  # [0,1] -> [-1,1]\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.FashionMNIST(\n",
        "    root=\"./data\", train=True, download=True, transform=train_transforms\n",
        ")\n",
        "test_dataset = torchvision.datasets.FashionMNIST(\n",
        "    root=\"./data\", train=False, download=True, transform=test_transforms\n",
        ")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader  = torch.utils.data.DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n",
        "\n",
        "idx_to_class = {i: name for i, name in enumerate(train_dataset.classes)}\n",
        "print(\"FashionMNIST classes:\", idx_to_class)\n"
      ],
      "metadata": {
        "id": "nJpHEHF9PPt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "▫️베이스라인 Generator/Discriminator\n",
        "\n",
        "- 베이스라인은 “조건부 생성”의 가장 단순한 형태를 택합니다.\n",
        "    - Generator:\n",
        "        - 입력: noise z + label embedding\n",
        "        - 출력: 1채널(흑백) 28x28 이미지\n",
        "\n",
        "- Discriminator:\n",
        "    - 입력: 이미지 + label 채널(또는 embedding을 채널로 확장)\n",
        "    - 출력: 진짜/가짜 확률(0~1) → Sigmoid + BCE\n",
        "\n",
        "- 이 구조의 장점:\n",
        "    - 구현이 쉽고 디버깅이 쉽다(미션 baseline으로 적합)\n",
        "\n",
        "- 단점:\n",
        "    - 구조가 얕으면 “선명도/다양성”이 한계가 있을 수 있음\n",
        "    - 조건 정보(레이블)가 이미지 생성에 충분히 강하게 박히지 않을 수 있음"
      ],
      "metadata": {
        "id": "yg3VZsYUPVzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 베이스라인 모델 정의(BCE버전)\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # WHY: 레이블을 벡터로 바꾸는 임베딩. one-hot보다 학습 유연성이 좋습니다.\n",
        "        self.label_emb = nn.Embedding(num_classes, 10)\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(latent_dim + 10, 128),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.Linear(128, 256),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            nn.Linear(256, image_size * image_size),\n",
        "            nn.Tanh()  # [-1,1] 범위로 맞춤\n",
        "        )\n",
        "\n",
        "    def forward(self, z, labels):\n",
        "        # torch.cat: 마지막 차원에서 (noise + label_vector) 연결\n",
        "        # WHY: 조건 정보를 \"입력 자체\"에 섞어주는 가장 단순한 방식\n",
        "        x = torch.cat([z, self.label_emb(labels)], dim=1)\n",
        "        out = self.model(x)\n",
        "        return out.view(-1, 1, image_size, image_size)\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # 레이블을 1차원으로 임베딩해서 이미지 채널처럼 붙이는 방식\n",
        "        self.label_emb = nn.Embedding(num_classes, 1)\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(1 + 1, 64, kernel_size=3, stride=2, padding=1),  # 28 -> 14\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),    # 14 -> 7\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128 * (image_size // 4) * (image_size // 4), 1),\n",
        "            nn.Sigmoid()  # BCE를 쓰기 때문에 확률로 변환\n",
        "        )\n",
        "\n",
        "    def forward(self, img, labels):\n",
        "        b = img.size(0)\n",
        "        label = self.label_emb(labels)         # (B,1)\n",
        "        label = label.view(b, 1, 1, 1)         # (B,1,1,1)\n",
        "        label = label.expand(b, 1, image_size, image_size)  # (B,1,28,28)\n",
        "        x = torch.cat([img, label], dim=1)     # (B,2,28,28)\n",
        "        return self.model(x)\n"
      ],
      "metadata": {
        "id": "n93_wBgNPVmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "▫️ 학습 루프(D(x), D(G(z))를 같이 보기)\n",
        "\n",
        "- GAN은 loss만 보면 감이 잘 안 옵니다. 그래서 보통 아래를 같이 봅니다.\n",
        "    - D(x): 진짜 이미지를 진짜라고 보는 평균 점수\n",
        "    - D(G(z)): 가짜를 진짜라고 보는 평균 점수\n",
        "\n",
        "- 이 값들이\n",
        "    - D(x)만 1.0에 가깝고 D(G(z))가 0에 박혀 있으면 → 판별자만 압승(생성자 학습이 막힘)\n",
        "    - 둘 다 0.5 근처에서 오래 머무르면 → 둘 다 “헷갈리기만 하고” 진짜 생성이 좋아졌다고 보기 애매"
      ],
      "metadata": {
        "id": "tBvWRhKmQSpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 베이스라인 학습 함수\n",
        "adversarial_loss = nn.BCELoss()\n",
        "\n",
        "def train_one_epoch(generator, discriminator,\n",
        "                    optimizer_G, optimizer_D,\n",
        "                    train_loader, device):\n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "\n",
        "    g_loss_sum = 0.0\n",
        "    d_loss_sum = 0.0\n",
        "    d_real_sum = 0.0\n",
        "    d_fake_sum = 0.0\n",
        "    n = 0\n",
        "\n",
        "    for imgs, labels in train_loader:\n",
        "        b = imgs.size(0)\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "        valid = torch.ones(b, 1, device=device)\n",
        "        fake  = torch.zeros(b, 1, device=device)\n",
        "\n",
        "\n",
        "        # Discriminator 학습\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        real_pred = discriminator(imgs, labels)\n",
        "        d_real_loss = adversarial_loss(real_pred, valid)\n",
        "\n",
        "        z = torch.randn(b, latent_dim, device=device)\n",
        "        gen_labels = torch.randint(0, num_classes, (b,), device=device)\n",
        "        gen_imgs = generator(z, gen_labels)\n",
        "\n",
        "        fake_pred = discriminator(gen_imgs.detach(), gen_labels)\n",
        "        d_fake_loss = adversarial_loss(fake_pred, fake)\n",
        "\n",
        "        d_loss = 0.5 * (d_real_loss + d_fake_loss)\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "\n",
        "        # Generator 학습\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        pred = discriminator(gen_imgs, gen_labels)\n",
        "        g_loss = adversarial_loss(pred, valid)       # \"가짜를 진짜로 속이는\" 목표\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        g_loss_sum += g_loss.item()\n",
        "        d_loss_sum += d_loss.item()\n",
        "        d_real_sum += real_pred.mean().item()\n",
        "        d_fake_sum += fake_pred.mean().item()\n",
        "        n += 1\n",
        "\n",
        "    return {\n",
        "        \"g_loss\": g_loss_sum / n,\n",
        "        \"d_loss\": d_loss_sum / n,\n",
        "        \"d_real\": d_real_sum / n,\n",
        "        \"d_fake\": d_fake_sum / n\n",
        "    }\n",
        "\n",
        "\n",
        "'''\n",
        "에폭을 많이 돌려도 “조건부로 잘 생성된다”는 확신이 약했다.\n",
        "학습 로그에서 D(x), D(G(z))가 크게 유의미하게 변하지 않는 구간이 길었다.\n",
        "즉, 생성자가 판별자를 “확실히 속이며 좋아진다”는 느낌이 약해서\n",
        "단순히 에폭만 늘리는 접근은 효율이 떨어질 수 있다.\n",
        "그래서 개선 실험에서는 “구조 자체 + 학습 안정화”를 동시에 바꿔서 해볼 예정\n",
        "'''"
      ],
      "metadata": {
        "id": "ox_Lcy3LQeGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "▫️개선 실험 계획\n",
        "\n",
        "- 시드 고정(비교 실험 필수)\n",
        "- 변수는 앞에서 한 번에 관리(실험 반복 편하게)\n",
        "- 데이터 증강은 “좌우반전만” 적용(과한 증강이 해가 될 수 있다고 판단)\n",
        "- Generator: label_dim=30 + ResNet 블록 사용\n",
        "- Discriminator: ResNet 블록 + Dropout 추가\n",
        "- 손실: BCE → MSE(LSGAN 스타일, Sigmoid 제거)\n",
        "- Optimizer: AdamW + weight_decay\n",
        "- Scheduler: CosineAnnealingLR\n",
        "- Label smoothing 적용(smoothing=0.1)\n",
        "- 체크포인트 저장 후 FID/IS로 정량 평가(가능한 범위에서)"
      ],
      "metadata": {
        "id": "0o0hXvZAPVir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 개선 실험 하이퍼파라미터\n",
        "image_size = 28\n",
        "num_classes = 10\n",
        "batch_size = 64\n",
        "num_epochs = 200\n",
        "latent_dim = 100\n",
        "\n",
        "label_dim = 30        # 레이블 임베딩 차원\n",
        "lr = 0.0001           # 개선 실험은 lr을 더 낮게 시작\n",
        "betas = (0.5, 0.999)\n",
        "weight_decay = 0.01\n",
        "\n",
        "dropout_p = 0.4\n",
        "smoothing = 0.1"
      ],
      "metadata": {
        "id": "mGpHD4VzSmIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "▫️transform / DataLoader\n",
        "- 개선 실험으로 증강을 최소화"
      ],
      "metadata": {
        "id": "2BWLDqMgStmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms_exp = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "test_transforms_exp = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset_exp = torchvision.datasets.FashionMNIST(\n",
        "    root=\"./data\", train=True, download=True, transform=train_transforms_exp\n",
        ")\n",
        "test_dataset_exp = torchvision.datasets.FashionMNIST(\n",
        "    root=\"./data\", train=False, download=True, transform=test_transforms_exp\n",
        ")\n",
        "\n",
        "train_loader_exp = torch.utils.data.DataLoader(train_dataset_exp, batch_size=batch_size, shuffle=True)\n",
        "test_loader_exp  = torch.utils.data.DataLoader(test_dataset_exp,  batch_size=batch_size, shuffle=False)\n",
        "\n",
        "idx_to_class_exp = {i: name for i, name in enumerate(train_dataset_exp.classes)}\n",
        "print(f\"FashionMNIST classes: {idx_to_class_exp}\")\n"
      ],
      "metadata": {
        "id": "V6lmMX18S2CG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🟨 ResNet 블록을 쓰는 이유\n",
        "\n",
        "- GAN에서 모델이 얕으면\n",
        "    - 디테일을 만드는 표현력이 부족하거나 학습이 흔들릴 때 회복이 어려울 수 있습니다.\n",
        "\n",
        "- ResNet(스킵 연결)의 장점:\n",
        "    - 깊게 쌓아도 기울기(gradient)가 덜 죽어서 학습이 비교적 안정적\n",
        "    - Generator에서 업샘플링을 단계적으로 하며 디테일을 만들기 좋음"
      ],
      "metadata": {
        "id": "PjAkPjRCTDpT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ResNet 기반 Generator/Discriminator\n",
        "\n",
        "class GeneratorResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            # WHY: Upsample로 공간 크기를 키운 뒤 Conv로 디테일을 만든다\n",
        "            nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False),\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "        )\n",
        "\n",
        "        # skip connection도 크기/채널을 맞춰줘야 더하기가 가능\n",
        "        self.shortcut = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False),\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.main(x) + self.shortcut(x)\n",
        "\n",
        "\n",
        "class DiscriminatorResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=2):\n",
        "        super().__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "\n",
        "        self.shortcut = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.main(x) + self.shortcut(x)\n",
        "\n",
        "\n",
        "class Generator_exp(nn.Module):\n",
        "    def __init__(self, latent_dim, num_classes, label_dim, image_size):\n",
        "        super().__init__()\n",
        "        self.init_size = image_size // 4  # 28 -> 7\n",
        "\n",
        "        self.label_emb = nn.Embedding(num_classes, label_dim)\n",
        "\n",
        "        self.l1 = nn.Sequential(\n",
        "            # WHY: noise와 label embedding을 연결해서 \"조건 정보\"를 초기에 강하게 주입\n",
        "            nn.Linear(latent_dim + label_dim, 128 * self.init_size * self.init_size),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.conv_blocks = nn.Sequential(\n",
        "            GeneratorResidualBlock(128, 128),  # 7 -> 14\n",
        "            GeneratorResidualBlock(128, 64),   # 14 -> 28\n",
        "        )\n",
        "\n",
        "        self.final_layer = nn.Sequential(\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 1, kernel_size=3, padding=1),\n",
        "            nn.Tanh()  # 데이터가 [-1,1]이므로 출력도 [-1,1]\n",
        "        )\n",
        "\n",
        "    def forward(self, noise, labels):\n",
        "        # dim=-1: 마지막 차원에 붙이겠다는 뜻 (벡터 연결 표준 패턴)\n",
        "        gen_input = torch.cat((noise, self.label_emb(labels)), dim=-1)\n",
        "\n",
        "        out = self.l1(gen_input)  # (B, 128*7*7)\n",
        "        # view: 텐서 모양만 바꾸는 reshape (데이터는 그대로, 형태만 변경)\n",
        "        out = out.view(out.size(0), 128, self.init_size, self.init_size)\n",
        "\n",
        "        out = self.conv_blocks(out)\n",
        "        img = self.final_layer(out)\n",
        "        return img\n",
        "\n",
        "\n",
        "class Discriminator_exp(nn.Module):\n",
        "    def __init__(self, num_classes, dropout_p, image_size):\n",
        "        super().__init__()\n",
        "        self.image_features = nn.Sequential(\n",
        "            DiscriminatorResidualBlock(1, 64, stride=2),   # 28 -> 14\n",
        "            DiscriminatorResidualBlock(64, 128, stride=2), # 14 -> 7\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "        # 레이블 임베딩을 판별자에도 입력으로 준다(조건부 판별)\n",
        "        self.label_embedding_dim_d = 50\n",
        "        self.label_embedding = nn.Embedding(num_classes, self.label_embedding_dim_d)\n",
        "\n",
        "        self.final_layer = nn.Sequential(\n",
        "            nn.Dropout(p=dropout_p),\n",
        "            # WHY: 이미지 특징 벡터 + 레이블 벡터를 concat해서 \"이 이미지가 이 레이블이 맞는지\" 판단하게 함\n",
        "            nn.Linear(128 * (image_size // 4) * (image_size // 4) + self.label_embedding_dim_d, 1)\n",
        "            # MSELoss(LSGAN)을 쓸 거라 Sigmoid는 넣지 않음\n",
        "        )\n",
        "\n",
        "    def forward(self, img, labels):\n",
        "        img_feats = self.image_features(img)\n",
        "        label_feats = self.label_embedding(labels)\n",
        "        combined = torch.cat([img_feats, label_feats], dim=1)\n",
        "        validity = self.final_layer(combined)\n",
        "        return validity\n"
      ],
      "metadata": {
        "id": "ahCPg1RcTQUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "▫️가중치 초기화 + 손실 / 옵티마이저 / 스케줄러"
      ],
      "metadata": {
        "id": "BLavhaqETcBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WHY: GAN은 초기 가중치에 따라 학습이 크게 흔들릴 수 있어서\n",
        "# DCGAN 계열에서 자주 쓰는 초기화(Conv: N(0,0.02), BN: N(1,0.02))를 적용\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find(\"BatchNorm2d\") != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "generator_exp = Generator_exp(\n",
        "    latent_dim=latent_dim,\n",
        "    num_classes=num_classes,\n",
        "    label_dim=label_dim,\n",
        "    image_size=image_size\n",
        ").to(device)\n",
        "\n",
        "discriminator_exp = Discriminator_exp(\n",
        "    num_classes=num_classes,\n",
        "    dropout_p=dropout_p,\n",
        "    image_size=image_size\n",
        ").to(device)\n",
        "\n",
        "generator_exp.apply(weights_init)\n",
        "discriminator_exp.apply(weights_init)\n",
        "\n",
        "# 손실: MSELoss (LSGAN 스타일)\n",
        "# WHY: BCE+Sigmoid는 D가 너무 확신하면 gradient가 약해질 수 있는데,\n",
        "# MSE 기반은 상대적으로 안정적인 경우가 많아서 실험으로 채택\n",
        "adversarial_loss_exp = nn.MSELoss().to(device)\n",
        "\n",
        "optimizer_G_exp = optim.AdamW(\n",
        "    generator_exp.parameters(),\n",
        "    lr=lr,\n",
        "    betas=betas,\n",
        "    weight_decay=weight_decay\n",
        ")\n",
        "\n",
        "optimizer_D_exp = optim.AdamW(\n",
        "    discriminator_exp.parameters(),\n",
        "    lr=lr,\n",
        "    betas=betas,\n",
        "    weight_decay=weight_decay\n",
        ")\n",
        "\n",
        "scheduler_G_exp = optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer_G_exp, T_max=num_epochs, eta_min=1e-6\n",
        ")\n",
        "scheduler_D_exp = optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer_D_exp, T_max=num_epochs, eta_min=1e-6\n",
        ")\n"
      ],
      "metadata": {
        "id": "QX-YNz2ZTfw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "▫️학습 함수(스무딩 + MSE 버전) + 체크포인트 저장"
      ],
      "metadata": {
        "id": "UrcoRPjIV7vR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch_exp(generator, discriminator,\n",
        "                        optimizer_G, optimizer_D,\n",
        "                        adversarial_loss,\n",
        "                        train_loader,\n",
        "                        device, latent_dim, num_classes, smoothing):\n",
        "    generator.train()\n",
        "    discriminator.train()\n",
        "\n",
        "    g_loss_sum = d_loss_sum = d_real_sum = d_fake_sum = 0.0\n",
        "    n = 0\n",
        "\n",
        "    for imgs, labels in train_loader:\n",
        "        b = imgs.size(0)\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "        # Label smoothing:\n",
        "        # WHY: D가 \"진짜=1.0\"에 과확신하는 걸 살짝 막아서 학습을 부드럽게 만들려는 목적\n",
        "        valid = torch.full((b, 1), 1.0 - smoothing, device=device, dtype=torch.float)\n",
        "        fake  = torch.full((b, 1), 0.0, device=device, dtype=torch.float)\n",
        "\n",
        "        z = torch.randn(b, latent_dim, device=device)\n",
        "        gen_labels = torch.randint(0, num_classes, (b,), device=device)\n",
        "        gen_imgs = generator(z, gen_labels)\n",
        "\n",
        "        # 1) Discriminator\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        real_validity = discriminator(imgs, labels)\n",
        "        d_real_loss = adversarial_loss(real_validity, valid)\n",
        "\n",
        "        fake_validity = discriminator(gen_imgs.detach(), gen_labels)\n",
        "        d_fake_loss = adversarial_loss(fake_validity, fake)\n",
        "\n",
        "        d_loss = 0.5 * (d_real_loss + d_fake_loss)\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # 2) Generator\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        validity = discriminator(gen_imgs, gen_labels)\n",
        "        # WHY: G는 D가 gen_imgs를 '진짜(0.9)'라고 보게 만들고 싶다\n",
        "        g_loss = adversarial_loss(validity, valid)\n",
        "\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        g_loss_sum += g_loss.item()\n",
        "        d_loss_sum += d_loss.item()\n",
        "        d_real_sum += real_validity.mean().item()\n",
        "        d_fake_sum += fake_validity.mean().item()\n",
        "        n += 1\n",
        "\n",
        "    return {\n",
        "        \"g_loss\": g_loss_sum / n,\n",
        "        \"d_loss\": d_loss_sum / n,\n",
        "        \"d_real\": d_real_sum / n,\n",
        "        \"d_fake\": d_fake_sum / n\n",
        "    }\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample_by_class(generator, device, latent_dim, num_classes, idx_to_class, n_per_class=8):\n",
        "    generator.eval()\n",
        "\n",
        "    # 클래스별로 n_per_class개씩 생성해서 그리드로 확인\n",
        "    z = torch.randn(num_classes * n_per_class, latent_dim, device=device)\n",
        "    labels = torch.tensor(\n",
        "        [c for c in range(num_classes) for _ in range(n_per_class)],\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    imgs = generator(z, labels)\n",
        "    imgs = (imgs + 1) / 2.0  # [-1,1] -> [0,1]로 보기 좋게\n",
        "\n",
        "    grid = make_grid(imgs, nrow=n_per_class)\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.imshow(grid.permute(1, 2, 0).cpu())\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Generated samples by class\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def train_cgan_exp(num_epochs,\n",
        "                   generator, discriminator,\n",
        "                   optimizer_G, optimizer_D,\n",
        "                   scheduler_G, scheduler_D,\n",
        "                   adversarial_loss,\n",
        "                   train_loader,\n",
        "                   device, latent_dim, num_classes, idx_to_class,\n",
        "                   smoothing,\n",
        "                   save_dir=\"./checkpoints_exp\",\n",
        "                   save_every=10):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        stats = train_one_epoch_exp(\n",
        "            generator, discriminator,\n",
        "            optimizer_G, optimizer_D,\n",
        "            adversarial_loss,\n",
        "            train_loader,\n",
        "            device, latent_dim, num_classes,\n",
        "            smoothing\n",
        "        )\n",
        "\n",
        "        # 학습 로그 (노트북 원본 출력 스타일 유지)\n",
        "        current_lr = optimizer_G.param_groups[0][\"lr\"]\n",
        "        print(\n",
        "            f\"Epoch [{epoch}/{num_epochs}] | \"\n",
        "            f\"G_loss: {stats['g_loss']:.4f} | D_loss: {stats['d_loss']:.4f} | \"\n",
        "            f\"D(x): {stats['d_real']:.4f} | D(G(z)): {stats['d_fake']:.4f} | \"\n",
        "            f\"LR: {current_lr:.6f}\"\n",
        "        )\n",
        "\n",
        "        # 중간 시각화\n",
        "        if epoch % 10 == 0:\n",
        "            sample_by_class(generator, device, latent_dim, num_classes, idx_to_class, n_per_class=8)\n",
        "\n",
        "        # 체크포인트 저장\n",
        "        if epoch % save_every == 0:\n",
        "            torch.save(generator.state_dict(), os.path.join(save_dir, f\"generator_exp_epoch_{epoch}.pth\"))\n",
        "            torch.save(discriminator.state_dict(), os.path.join(save_dir, f\"discriminator_exp_epoch_{epoch}.pth\"))\n",
        "            print(f\"Saved models for epoch {epoch}\")\n",
        "\n",
        "        # 스케줄러는 에폭 끝에서 step\n",
        "        scheduler_G.step()\n",
        "        scheduler_D.step()\n"
      ],
      "metadata": {
        "id": "NytBfoFyWBtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "▫️ 개선 실험 학습 실행"
      ],
      "metadata": {
        "id": "NEnX74DHWG3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_cgan_exp(\n",
        "    num_epochs=num_epochs,\n",
        "    generator=generator_exp,\n",
        "    discriminator=discriminator_exp,\n",
        "    optimizer_G=optimizer_G_exp,\n",
        "    optimizer_D=optimizer_D_exp,\n",
        "    scheduler_G=scheduler_G_exp,\n",
        "    scheduler_D=scheduler_D_exp,\n",
        "    adversarial_loss=adversarial_loss_exp,\n",
        "    train_loader=train_loader_exp,\n",
        "    device=device,\n",
        "    latent_dim=latent_dim,\n",
        "    num_classes=num_classes,\n",
        "    idx_to_class=idx_to_class_exp,\n",
        "    smoothing=smoothing\n",
        ")\n",
        "\n",
        "'''\n",
        "FID(Fréchet Inception Distance)\n",
        "- “실제 이미지 분포”와 “생성 이미지 분포”의 거리\n",
        "- 낮을수록 좋다(가까울수록 진짜와 비슷)\n",
        "\n",
        "IS(Inception Score)\n",
        "- 생성 이미지가 선명한가(품질)\n",
        "- 생성 이미지가 다양하게 나오나(다양성)\n",
        "- 높을수록 좋다\n",
        "'''"
      ],
      "metadata": {
        "id": "u-UmxT-uWJY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "▫️FID/IS 계산 + 체크포인트 자동 평가"
      ],
      "metadata": {
        "id": "hk8li8wzWbwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from torchvision.models import inception_v3\n",
        "from scipy.linalg import sqrtm\n",
        "import glob\n",
        "\n",
        "# Inception 모델 로드(처음 1회만)\n",
        "inception_model_fid = inception_v3(pretrained=True, transform_input=False).to(device)\n",
        "inception_model_fid.fc = nn.Identity()  # feature extractor로 사용\n",
        "inception_model_fid.eval()\n",
        "\n",
        "inception_model_is = inception_v3(pretrained=True, transform_input=False).to(device)\n",
        "inception_model_is.eval()\n",
        "\n",
        "def get_inception_features(images, model, batch_size=32, device=device):\n",
        "    model.eval()\n",
        "    n_images = images.shape[0]\n",
        "    features = []\n",
        "\n",
        "    # Inception 입력 정규화(일반적인 ImageNet mean/std)\n",
        "    norm_mean = torch.tensor([0.485, 0.456, 0.406], device=device).view(1,3,1,1)\n",
        "    norm_std  = torch.tensor([0.229, 0.224, 0.225], device=device).view(1,3,1,1)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, n_images, batch_size):\n",
        "            batch = images[i:i+batch_size]\n",
        "\n",
        "            # FashionMNIST는 1채널이므로 3채널로 복제\n",
        "            if batch.shape[1] == 1:\n",
        "                batch = batch.repeat(1,3,1,1)\n",
        "\n",
        "            # Inception 입력 크기 299x299로 업샘플링\n",
        "            batch = F.interpolate(batch, size=(299,299), mode=\"bilinear\", align_corners=False)\n",
        "            batch = (batch - norm_mean) / norm_std\n",
        "\n",
        "            pred = model(batch)\n",
        "            features.append(pred.cpu().numpy())\n",
        "\n",
        "    return np.concatenate(features, axis=0)\n",
        "\n",
        "\n",
        "def calculate_fid(real_images, generated_images, model, batch_size=32, device=device):\n",
        "    feat_real = get_inception_features(real_images, model, batch_size, device)\n",
        "    feat_gen  = get_inception_features(generated_images, model, batch_size, device)\n",
        "\n",
        "    mu_real, sigma_real = feat_real.mean(axis=0), np.cov(feat_real, rowvar=False)\n",
        "    mu_gen,  sigma_gen  = feat_gen.mean(axis=0),  np.cov(feat_gen,  rowvar=False)\n",
        "\n",
        "    covmean, _ = sqrtm(sigma_real.dot(sigma_gen), disp=False)\n",
        "    if np.iscomplexobj(covmean):\n",
        "        covmean = covmean.real\n",
        "\n",
        "    fid = np.sum((mu_real - mu_gen) ** 2) + np.trace(sigma_real + sigma_gen - 2 * covmean)\n",
        "    return float(fid)\n",
        "\n",
        "\n",
        "def calculate_inception_score(images, model, batch_size=32, splits=10, device=device):\n",
        "    model.eval()\n",
        "    n_images = images.shape[0]\n",
        "\n",
        "    # Inception은 1000-class softmax 출력이므로 확률 분포를 얻는다\n",
        "    preds = []\n",
        "\n",
        "    norm_mean = torch.tensor([0.485, 0.456, 0.406], device=device).view(1,3,1,1)\n",
        "    norm_std  = torch.tensor([0.229, 0.224, 0.225], device=device).view(1,3,1,1)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, n_images, batch_size):\n",
        "            batch = images[i:i+batch_size]\n",
        "            if batch.shape[1] == 1:\n",
        "                batch = batch.repeat(1,3,1,1)\n",
        "            batch = F.interpolate(batch, size=(299,299), mode=\"bilinear\", align_corners=False)\n",
        "            batch = (batch - norm_mean) / norm_std\n",
        "\n",
        "            logits = model(batch)\n",
        "            prob = torch.softmax(logits, dim=1)\n",
        "            preds.append(prob.cpu().numpy())\n",
        "\n",
        "    preds = np.concatenate(preds, axis=0)\n",
        "\n",
        "    # IS 계산(분할 평균)\n",
        "    scores = []\n",
        "    split_size = n_images // splits\n",
        "    for k in range(splits):\n",
        "        part = preds[k * split_size : (k + 1) * split_size]\n",
        "        py = np.mean(part, axis=0)\n",
        "        kl = part * (np.log(part + 1e-10) - np.log(py + 1e-10))\n",
        "        kl = np.sum(kl, axis=1)\n",
        "        scores.append(np.exp(np.mean(kl)))\n",
        "\n",
        "    return float(np.mean(scores)), float(np.std(scores))\n",
        "\n",
        "\n",
        "# 실제 이미지 5000장 준비(노트북 원본 출력 흐름)\n",
        "real_imgs = []\n",
        "for imgs, _ in test_loader_exp:\n",
        "    real_imgs.append(imgs)\n",
        "    if sum(x.size(0) for x in real_imgs) >= 5000:\n",
        "        break\n",
        "real_imgs = torch.cat(real_imgs, dim=0)[:5000].to(device)\n",
        "real_imgs = (real_imgs + 1) / 2.0  # [0,1]\n",
        "\n",
        "print(\"--- 모든 체크포인트 자동 평가 시작 ---\")\n",
        "print(\"  - 5000개의 실제 이미지를 평가용으로 준비했습니다.\")\n",
        "\n",
        "# 체크포인트 목록\n",
        "ckpts = sorted(glob.glob(\"./checkpoints_exp/generator_exp_epoch_*.pth\"))\n",
        "print(f\"--- 총 {len(ckpts)}개의 체크포인트를 평가합니다. ---\\n\")\n",
        "\n",
        "results = {}\n",
        "for path in ckpts:\n",
        "    epoch = int(path.split(\"_\")[-1].split(\".\")[0])\n",
        "    print(f\"Evaluating checkpoint: {os.path.basename(path)} (Epoch {epoch})\")\n",
        "\n",
        "    # 모델 로드\n",
        "    gen = Generator_exp(latent_dim, num_classes, label_dim, image_size).to(device)\n",
        "    gen.load_state_dict(torch.load(path, map_location=device))\n",
        "    gen.eval()\n",
        "\n",
        "    # 생성 이미지 5000장 만들기\n",
        "    n_gen = 5000\n",
        "    z = torch.randn(n_gen, latent_dim, device=device)\n",
        "    labels = torch.randint(0, num_classes, (n_gen,), device=device)\n",
        "    gen_imgs = gen(z, labels)\n",
        "    gen_imgs = (gen_imgs + 1) / 2.0  # [0,1]\n",
        "\n",
        "    fid = calculate_fid(real_imgs, gen_imgs, inception_model_fid, batch_size=32, device=device)\n",
        "    is_score, is_std = calculate_inception_score(gen_imgs, inception_model_is, batch_size=32, splits=10, device=device)\n",
        "\n",
        "    print(f\"  -> 결과: FID = {fid:.4f}, IS = {is_score:.4f} ± {is_std:.4f}\\n\")\n",
        "    results[epoch] = {\"fid\": fid, \"is\": is_score, \"is_std\": is_std, \"path\": path}\n"
      ],
      "metadata": {
        "id": "Xva0cdhvWcnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🟨 최종 결론 및 분석\n",
        "\n",
        "## 1. 실험 요약\n",
        "본 실험은 FashionMNIST에서 클래스 레이블을 조건으로 이미지를 생성하는 cGAN을 구현하고,\n",
        "Baseline 재현 이후 구조 및 학습 안정화 기법을 적용하여 생성 품질 변화를 비교·분석하였다.\n",
        "\n",
        "- Baseline: 단순 CNN 기반 Generator/Discriminator, BCE 기반 손실, 조건 결합(concat) 방식\n",
        "- 개선 실험: Residual block 기반 구조, LSGAN 스타일(MSE), AdamW + weight decay, CosineAnnealingLR,\n",
        "  label smoothing 및 dropout 등 안정화 기법을 포함\n",
        "\n",
        "---\n",
        "\n",
        "## 2. 정량 결과(기록 기준)\n",
        "체크포인트 자동 평가 결과(기록된 일부)에서,\n",
        "학습이 진행될수록 FID가 전반적으로 감소하는 경향을 확인하였다.\n",
        "- Epoch 10: FID 39.4373, IS 4.0799 ± 0.1412\n",
        "- Epoch 100: FID 16.8721, IS 4.5606 ± 0.1323\n",
        "- Epoch 110: FID 18.6463, IS 4.3996 ± 0.1291\n",
        "\n",
        "해석:\n",
        "- 최소한 “생성 이미지 분포가 실제 이미지 분포에 가까워지는 방향(FID 감소)”은 관찰되었다.\n",
        "- 단, FashionMNIST는 자연이미지 도메인과 달라 IS/FID의 절대값보다는 “실험 간 상대 비교”에 의미가 크다.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. 핵심 인사이트\n",
        "(1) 품질 지표 개선은 ‘학습 안정화 패키지’가 만든 결과일 가능성이 크다  \n",
        "- MSE(LSGAN) + AdamW + 코사인 스케줄 + smoothing/dropout 조합은\n",
        "  판별자가 과확신하거나 학습이 급격히 불안정해지는 현상을 완화하는 방향으로 작동한다.\n",
        "- 결과적으로 체크포인트가 진행될수록 FID가 낮아지는 흐름을 만들었다.\n",
        "\n",
        "(2) cGAN의 본질은 “조건 일치”이며, 여기서 추가 개선이 필요하다  \n",
        "- 정량 지표가 좋아져도, 레이블과 생성 이미지가 일치하지 않는 사례가 존재한다면\n",
        "  조건부 생성의 목표를 충분히 달성했다고 보기 어렵다.\n",
        "- 따라서 다음 개선의 중심은 “조건 정보 주입을 더 강하게/정확하게 만드는 방법”이어야 한다.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. 한계 및 다음 실험 방향\n",
        "- 조건 주입 방식 고도화:\n",
        "  - 단순 concat을 넘어 projection discriminator, Conditional BatchNorm, FiLM 등으로 확장 검토\n",
        "- 안정화 기법 재조정:\n",
        "  - label smoothing 강도, dropout 비율, lr 범위를 스윕하여 최적 조합 탐색\n",
        "- 평가 프로토콜 고정:\n",
        "  - 매 N epoch마다 “클래스별 고정 그리드”를 생성해 레이블 일치 여부를 정성 평가 루틴으로 고정\n",
        "\n",
        "---\n",
        "\n",
        "## 5. 결론\n",
        "FashionMNIST 조건부 생성 문제에서 cGAN을 구현하고, 구조/학습 안정화 기법을 통해 생성 품질 개선 흐름(FID 감소)을 확인하였다. 향후에는 정량 지표뿐 아니라 “레이블 조건이 일관되게 반영되는지”를 최우선 목표로 두고, 조건 주입 방식의 구조적 개선을 중심으로 추가 실험을 진행하는 것이 타당할것으로 보임\n"
      ],
      "metadata": {
        "id": "SczY0mEPWi6k"
      }
    }
  ]
}